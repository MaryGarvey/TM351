{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "source": [
    "# MongoDB Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-student"
    ]
   },
   "source": [
    "## MongoDB and CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the UK Baby Names dataset introduced in my TMA01 Preparation Tutorial (available on Github: https://github.com/MaryGarvey/TM351). The second half of the Notebook looks at using JSON data. \n",
    "\n",
    "Activity 13.2 introduces *Seven Databases in Seven Weeks* (Redmond 2012)\n",
    "\n",
    "The most common NoSQL databases introduced are:\n",
    "\n",
    "- Riak\t- key value\n",
    "- Hbase\t- wide column\n",
    "- MongoDB\t- document \n",
    "- CouchDB - document \n",
    "- Neo4j\t- graph\n",
    "- Redis\t- key value\n",
    "\n",
    "This notebook will look at the MongoDB NoSQL document database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK Baby Names ðŸ‘¶ (1996-2021)\n",
    "\n",
    "## Introduction (from the Kaggle Website)\n",
    "\n",
    "<i>Baby name statistics are compiled from first names recorded when live births are registered in England and Wales as part of civil registration, a legal requirement.\n",
    "The statistics are based only on live births which occurred in the calendar year, as there is no public register of stillbirths.</i>\n",
    "\n",
    "<i>Babies born in England and Wales to women whose usual residence is outside England and Wales are included in the statistics for England and Wales as a whole, but excluded from any sub-division of England and Wales.\n",
    "The statistics are based on the exact spelling of the name given on the birth certificate. Grouping names with similar pronunciation would change the rankings. Exact names are given so users can group if they wish.</i>\n",
    "\n",
    "<i>The dataset contains records of around 16k boy names and 22k girl names.</i>\n",
    "\n",
    "You can get further information and the datasets from: \n",
    "https://www.kaggle.com/datasets/johnsmith44/uk-baby-names-1996-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "import pymongo\n",
    "import datetime\n",
    "import collections\n",
    "#import Object\n",
    "\n",
    "import pandas as pd\n",
    "# better for printing JSON data: p(retty)print\n",
    "from pprint import pprint\n",
    "\n",
    "# Print out the version of pymongo \n",
    "print (pymongo.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET DATABASE CONNECTION STRINGS\n",
    "MONGOHOST='localhost'\n",
    "MONGOPORT=27017\n",
    "MONGOCONN='mongodb://{MONGOHOST}:{MONGOPORT}/'.format(MONGOHOST=MONGOHOST,MONGOPORT=MONGOPORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB version\n",
    "! mongod --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(MONGOCONN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the tutorial databases so we start with a clean sheet\n",
    "# Unlike SQL, the command will not generate an error if it does not already exist\n",
    "client.drop_database('babyNamesDB')\n",
    "client.drop_database('politicsDB')\n",
    "client.drop_database('twitterDB')\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the start and end of the file for any issues\n",
    "!head data/UKGirlNames1996-2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail data/UKGirlNames1996-2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# babyNamesDB is a database that contains 2 collections (similar to tables)\n",
    "db = client.babyNamesDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to import the CSV dataset.\n",
    "\n",
    "- use the `mongoimport` command\n",
    "- import into a dataframe as normal, then convert to a MongoDB collection\n",
    "\n",
    "Both methods will be shown here for information.\n",
    "\n",
    "1. Using mongoimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. using mongoimport\n",
    "! mongoimport --db babyNamesDB --type=csv --headerline --file data/UKGirlNames1996-2021.csv --collection girls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. importing via a data frame\n",
    "names_df = pd.read_csv(\"data/UKBoyNames1996-2021.csv\")\n",
    "db.boys.insert_many(names_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the database has been added (babyNamesDB)\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and it contains the two collections\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup variables for the two collections\n",
    "boys = db.boys\n",
    "girls = db.girls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many documents does each collection have:\n",
    "print(\"Girls:\\t{}\".format(girls.count_documents({})))\n",
    "print(\"Boys:\\t{}\".format(boys.count_documents({})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables saves us having to use db.collectionName.function() in the queries, for example, you can use `girls.find()` instead of `db.girls.find()`. You can still use the longer format.\n",
    "\n",
    "Just be careful if you swap databases in the same Notebook, as we do later, you could end up referencing a collection in the wrong database. Mongo will not warn you that this is an error, it just assumes it does not exist and will return nothing - a consequence of a schemaless database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one record - can be any one from the collection\n",
    "girls.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are a lot of missing values, which will be removed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "source": [
    "# Querying "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB data is stored in JSON format, which means it uses the format of: *{key: value}* for most things.\n",
    "\n",
    "The *find()* function is the equivalent of the SQL SELECT statement.\n",
    "\n",
    "Instead of a *WHERE* clause you need to provide a JSON string for what you want to find.\n",
    "\n",
    "For example, the following is the equivalent of *SELECT * FROM girlsName WHERE name = 'Mary';*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.find({'Name': 'Mary'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can specify a search criteria with find_one too (could be the only one)\n",
    "girls.find_one({'Name': 'Mary-Beth'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `find()` and `find_one()` is that the former returns all the documents matching the criteria, whereas the latter returns just one of the documents, which can be used to the structure of the data. Do bear in mind, since MongoDB can store semi-structured data, different documents could have a different structure, unlike a relational database, where records in a table would all have the same structure.\n",
    "\n",
    "To see what is returned in the cursor, lets create some functions to print the individual documents from the cursor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means an iterator is needed to display the results\n",
    "# using pretty print\n",
    "def printDocs(documents):\n",
    "    for doc in documents:\n",
    "        pprint(doc)\n",
    "\n",
    "# ordinary print\n",
    "def printDoc(documents):\n",
    "    for doc in documents:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the Marys\n",
    "docs = girls.find({'Name': 'Mary'})\n",
    "printDoc(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alternatively use a dataframe to make it more like a relational table\n",
    "# find the girls names in 2021 with a count more than 2000\n",
    "pd.DataFrame(girls.find({\"2021 Count\" : {\"$gt\": 2000}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or can convert the Cursor to a list\n",
    "# find the girls names in 2021 with a count more than 2000, but were ranked in the top 10 in 2020\n",
    "list(girls.find({\"2021 Count\" : {\"$gt\": 2000}, \"2020 Rank\": {\"$lte\": 10}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "source": [
    "# Data Dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One consequence of being schemaless, means there are no conventional data dictionary tables to check if the collection or document names exist. This means that it will not generate an error message if neither exist. Do note, the names are all case sensitive. \n",
    "\n",
    "Why will the following return no records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.find_one({\"Name\" : \"Fred\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.girls.find_one({\"name\" : \"Susan\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it will generate an error message if it can not find the variables or functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Girls.find_one({\"Name\" : \"Susan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.find_One({\"Name\" : \"Susan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.find_One({\"Name\" : Susan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.find_One({Name : \"Susan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets find our girl\n",
    "girls.find_one({\"Name\" : \"Susan\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may not be a data dictionary collection to query, but you can find the keys in a collection, which are similar to the column names in a relational database. Be aware though, that the structure can vary from document to document in a given collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.find_one().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen previously there are a lot of fields with no data. One good point for a NoSQL database is that every document does not have to have the same structure, so if the value is blank, there is no need to store the key.\n",
    "\n",
    "For example, lets remove any records where the \"2021 Rank\" is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.update_many({\"2021 Rank\" : \"\"}, { \"$unset\": {\"2021 Rank\" : 1 }});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the amount of empty keys, it would be tedious to remove each one separately, so lets find what keys each record has and then loop through removing any blanks.\n",
    "\n",
    "Do note, `find_one()` could retrieve any record, if the data was semi-structured each document could have a different structure. In this case, the data came from a CSV file, so every document has the same structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = girls.find_one({}).keys()\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keys:\n",
    "    girls.update_many({ k : \"\"}, { \"$unset\": { k : 1 }});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, the above has removed any empty keys, but the document will still exist\n",
    "girls.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same to the boys names\n",
    "keys = boys.find_one({}).keys()\n",
    "for k in keys:\n",
    "    boys.update_many({ k : \"\"}, { \"$unset\": { k : 1 }});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boys.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The consequence of this is that the keys will be slightly different for the records that have more complete data\n",
    "# one with sparse data\n",
    "girls.find_one({\"Name\" : \"Marvi\"}).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more complete:\n",
    "girls.find_one({\"Name\" : \"Martina\"}).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many documents in the collection\n",
    "db.girls.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# can access via the index (starts at 0)\n",
    "girls.find()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second record\n",
    "girls.find()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last one\n",
    "len = girls.count_documents({})-1\n",
    "girls.find()[len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`count_documents()` can be used with queries to count the result, rather than listing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls.count_documents({\"Name\": \"Mary\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many documents have a count more than 1500 in 2021\n",
    "girls.count_documents({\"2021 Count\": {\"$gt\" : 1500} })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "source": [
    "# Part 15: Complex queries and analysis\n",
    "# Aggregation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex processing, including grouping, aggregation functions, and data renaming is achieved through MongoDBâ€™sÂ aggregation pipeline.\n",
    "\n",
    "For example a query can involve several stages:\n",
    "                                                \n",
    "First stage: filter out documents that do not match some criterion<br>\n",
    "Second stage: group those documents<br>\n",
    "Third stage: select only groups that match another criterion<br>\n",
    "Fourth stage: group summaries would then be returned to the client<br>\n",
    "\n",
    "By building up a pipeline in stages, complex data processing tasks can be built from simple components.\n",
    "\n",
    "<img src=\"pipeline.png\">\n",
    "\n",
    "<img src=\"pipeline_functions.png\">\n",
    "\n",
    "Further examples can be found in *Notebook 15.3 Introducing aggregation pipelines.*\n",
    "\n",
    "The examples below and in the practical activities all use small data sets that can be used locally. With huge datasets, the processing may be spread over many computers for processing to aid speed. Data processing tools (such as the aggregation pipeline and MapReduce) keep the processing of data near that data itself, reducing the work required by the client and the amount of data to be moved across the network from server to client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to SELECT COUNT(*) FROM girls;\n",
    "# Need to group by an _id\n",
    "pipeline = [\n",
    "     {\"$group\": {\"_id\": 0, \"Name\": {\"$sum\": 1}}},\n",
    "]\n",
    "\n",
    "list(girls.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT \"2021 Count\", count(*) FROM training ORDER BY \"2021 Count\";\n",
    "printDoc(db.girls.aggregate( [ { \"$group\" : { \"_id\" : \"$2021 Count\", \"count\": {\"$sum\": 1} }},\n",
    "                               { \"$sort\" : {\"_id\" : 1}} ] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT Name, count(*) FROM girls;\n",
    "printDoc(db.girls.aggregate( [ { \"$group\" : { \"_id\" : \"$Name\", \"count\": {\"$sum\": 1} }} ] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-student"
    ]
   },
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do statistics on this data we want to use information in the keys as values, e.g., extract the year from: `2020 Count`. In Tutorial 2 we did some processing to do this, so lets reuse the code to reshape our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFile(fileType):\n",
    "    # remove missing data permanately\n",
    "    filename = 'data/UK'+fileType+'Names1996-2021.csv'\n",
    "    print(\"Importing: '\"+filename+\"'\")\n",
    "    names_df = pd.read_csv(filename)\n",
    "    names_df = names_df.dropna(how='any')\n",
    "    # unpivot the dataframe from a wide to long format\n",
    "    names2_df = pd.melt(names_df, id_vars=\"Name\")\n",
    "    # split the two values in variable: year and the type (count or rank)\n",
    "    names2_df[['Year','Type']] = names2_df['variable'].str.split(' ', expand = True)\n",
    "    # convert year to a number\n",
    "    names2_df['Year'] = names2_df['Year'].astype(str).astype(int)\n",
    "    # the variable column is no longer needed\n",
    "    names2_df.drop('variable', axis=1, inplace=True)\n",
    "    names2_df.head()\n",
    "    # save the changes \n",
    "    names2_df.to_csv('data/'+fileType+'Updated.csv')\n",
    "    return names2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boys_df = updateFile(\"Boy\")\n",
    "boys_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls_df = updateFile(\"Girl\")\n",
    "girls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next import the girls_df dataframe into a collection (girlsUpdate)\n",
    "db.girlsUpdate.insert_many(girls_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ditto the boys_df dataframe\n",
    "db.boysUpdate.insert_many(boys_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check they are now in the baby names database (babyNamesDB)\n",
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.girlsUpdate.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.boysUpdate.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many documents does each collection have:\n",
    "print(\"Girls: \\t\\t{}\".format(girls.count_documents({})))\n",
    "print(\"Girls Update: \\t{}\".format(db.girlsUpdate.count_documents({})))\n",
    "print(\"Boys: \\t\\t{}\".format(boys.count_documents({})))\n",
    "print(\"Boys Update: \\t{}\".format(db.boysUpdate.count_documents({})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT Year, count(*) as count FROM boysUpdate;\n",
    "printDoc(db.boysUpdate.aggregate( [ { \"$group\" : { \"_id\" : \"$Year\", \"count\": {\"$sum\": 1} }} ] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT Year, sum() as \"Sum of Values\" FROM boysUpdate GROUP BY Year ORDER BY Year (_id) descending;\n",
    "printDoc(db.boysUpdate.aggregate( [ { \"$group\" : { \"_id\" : \"$Year\", \"Sum of values\": {\"$sum\": \"$value\"}}},\n",
    "                                                 { \"$sort\" : {\"_id\" : -1}}  \n",
    "                                     ] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT Name, avg(value) as \"Average Rank\" FROM girlsUpdate WHERE Type = 'Rank' ORDER BY \"Average Rank\";\n",
    "# This pipeline involves 3 stages: $match, $group and $sort\n",
    "printDoc(db.girlsUpdate.aggregate( [ { \"$match\" : {\"Type\": \"Rank\"} },\n",
    "                                     { \"$group\" : { \"_id\" : \"$Name\", \"Average Rank\": {\"$avg\": \"$value\"} }},\n",
    "                                     { \"$sort\" : {\"Average Rank\" : 1}}\n",
    "                                     ] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining documents was not possible in earlier versions of MongoDB, later versions introduced something similar to a simple left join using the pipeline `$lookup` operator ([Mongo docs](https://www.mongodb.com/docs/manual/reference/operator/aggregation/lookup/)).\n",
    "\n",
    "MongoDB provides the joins as part of the aggregation steps. \n",
    "\n",
    "These examples use the boy and girl collections, you still join on a common column as seen in relational databases. \n",
    "\n",
    "In this case *Name* is the common field in both collections.\n",
    "\n",
    "First, convert the collections to a dataframe and let us do a quick check if there are common names in the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boys_df = pd.DataFrame(boys.find())\n",
    "girls_df = pd.DataFrame(girls.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boys_df[boys_df['Name'].isin(girls_df['Name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girls_df[girls_df['Name'].isin(boys_df['Name'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be quite a few matching names in both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if Alex appears in both, plus only show some fields\n",
    "girls.find_one({\"Name\" : \"Alex\"}, {\"_id\":0, \"Name\": 1, \"2021 Rank\": 1, \"2021 Count\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boys.find_one({\"Name\" : \"Alex\"}, {\"_id\":0, \"Name\": 1, \"2021 Rank\": 1, \"2021 Count\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default MongoDB will carry out an outer join, which means the names that do not match will contain an empty subdocument. Really we want an inner join, `as` creates an array, or subdocument within each document, further work can be done on the `as` array to pull out just the arrays that are not empty.\n",
    "\n",
    "See https://www.mongodb.com/docs/manual/reference/operator/aggregation/unwind/ for further information on the `preserveNullAndEmptyArrays` field.\n",
    "\n",
    "Thanks to https://stackoverflow.com/questions/37575722/how-to-do-inner-joining-in-mongodb for an example (accessed 08/01/2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(girls.aggregate([\n",
    "   {\n",
    "     \"$lookup\":\n",
    "       {\n",
    "         \"from\": \"boys\",\n",
    "         \"localField\": \"Name\",\n",
    "         \"foreignField\": \"Name\",\n",
    "         \"as\": \"joined\"        \n",
    "       }\n",
    "   }, \n",
    "    {\"$unwind\": {\n",
    "           \"path\": \"$joined\",\n",
    "           \"preserveNullAndEmptyArrays\": False\n",
    "   }}    \n",
    "  ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "source": [
    "# Semi-Structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baby Names dataset is an example of structured data, in that it is very uniform, with the same data types in each column.\n",
    "\n",
    "The power of NoSQL databases is in copying semi-structured data, such as JSON data, where the values may not be straightforward strings and numbers, but could be nested documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-student"
    ]
   },
   "source": [
    "## USA Government data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of publicly available data is in JSON format, for example, government agencies:\n",
    "\n",
    "- https://catalog.data.gov/dataset?res_format=JSON\n",
    "\n",
    "- https://github.com/jdorfman/awesome-json-datasets#government\n",
    "\n",
    "Below uses the USA government politician datasets found in the last link.\n",
    "\n",
    "- Current US Senators: roles.json\n",
    "- Current US Representatives: role-reps.json\n",
    "\n",
    "Plus a list of USA States and abbreviations:\n",
    "- states_titlecase.json\n",
    "\n",
    "found here: https://gist.github.com/mshafrir/2646763\n",
    "\n",
    "All downloaded: 09/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the data\n",
    "!head data/role.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail data/role.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/role-reps.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail data/role-reps.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, this file was amended to remove the commas between each document (otherwise would not import)\n",
    "!head data/states_titlecase.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail data/states_titlecase.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The politicians data looks to be in JSON format and appear to have some meta data at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_database('politicsDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data using mongoimport, note the type is now json\n",
    "! mongoimport --db politicsDB --type=json --file data/role.json  --collection senators\n",
    "! mongoimport --db politicsDB --type=json --file data/role-reps.json --collection reps\n",
    "! mongoimport --db politicsDB --type=json --file data/states_titlecase.json --collection states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change database\n",
    "db = client.politicsDB\n",
    "senators = db.senators\n",
    "reps = db.reps\n",
    "states = db.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a document in each collection\n",
    "senators.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the politician details have nested documents, where a document (or array) is nested within other information. This is an example of semi-structured data.\n",
    "\n",
    "The dot syntax can be used to search nested documents. For example, a snippet of information from above for the person sub-document, shows what keys are available within it: \n",
    "\n",
    "<pre>\n",
    "person': {'bioguideid': 'A000055',\n",
    "    'birthday': '1965-07-22',\n",
    "    'cspanid': 45516,\n",
    "    'fediverse_webfinger': None,\n",
    "    'firstname': 'Robert',\n",
    "    'gender': 'male',\n",
    "    'gender_label': 'Male',\n",
    "    'lastname': 'Aderholt',\n",
    "    'link': 'https://www.govtrack.us/congress/members/robert_aderholt/400004',\n",
    "    'middlename': 'B.',\n",
    "    'name': 'Rep. Robert Aderholt [R-AL4]',\n",
    "    'namemod': '',\n",
    "    'nickname': '',\n",
    "    'osid': 'N00003028',\n",
    "    'pvsid': None,\n",
    "    'sortname': 'Aderholt, Robert (Rep.) [R-AL4]',\n",
    "    'twitterid': 'Robert_Aderholt',\n",
    "    'youtubeid': 'RobertAderholt'},\n",
    "</pre>    \n",
    "\n",
    "To pull out the full information for this representative, I'm assuming :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps.find_one({\"objects.person.bioguideid\": 'A000055' })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, this has found the representative, but the consequence of all the politicians being stored in one document, rather than one document per politician is that if the query returns true, then all the data in that document is returned!\n",
    "\n",
    "To extract items from the array requires the use of the `$unwind` operator.\n",
    "\n",
    "For example, display just the firstnames, suppressing the generated id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reps.aggregate([{\"$project\" : {\"_id\": 0, \"objects.person.firstname\" : 1}},\n",
    "                        {\"$unwind\":\"$objects\"} ]) \n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reps.aggregate([{\"$project\" : {\"_id\": 0, \"objects.person.firstname\" : 1, \"objects.person.lastname\": 1,\n",
    "                                     \"objects.person.bioguideid\":1}},\n",
    "                       {\"$unwind\":\"$objects\"},\n",
    "                       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}\n",
    "                         ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do make sure the pipeline is in the right order, if the match is done too soon it will again return all the representatives if the query criteria is matched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reps.aggregate([{\"$project\" : {\"_id\": 0, \"objects.person.firstname\" : 1, \"objects.person.lastname\": 1,\n",
    "                                     \"objects.person.bioguideid\":1}},\n",
    "                       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}, \n",
    "                       {\"$unwind\":\"$objects\"}\n",
    "                      ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or if in doubt duplicate the $match as discussed here:\n",
    "# https://stackoverflow.com/questions/54030089/how-to-use-unwind-and-match-with-mongodb\n",
    "\n",
    "docs = reps.aggregate([{\"$project\" : {\"_id\": 0, \"objects.person.firstname\" : 1, \"objects.person.lastname\": 1,\n",
    "                                     \"objects.person.bioguideid\":1}},\n",
    "                       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}, \n",
    "                       {\"$unwind\":\"$objects\"},\n",
    "                       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}\n",
    "                         ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the person details, which does need a duplicate $match \n",
    "docs = reps.aggregate([{\"$project\" : {\"_id\": 0, \"objects.person\" : 1}},\n",
    "                       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}, \n",
    "                       {\"$unwind\":\"$objects\"},\n",
    "                       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}\n",
    "                      ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the person details, which does need a duplicate $match \n",
    "docs = reps.aggregate([{\"$project\" : {\"_id\": 0, \"objects.person\" : 1}},\n",
    "                       {\"$unwind\":\"$objects\"},\n",
    "                       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}\n",
    "                      ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "person is part of the details stored for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the details for this representative \n",
    "docs = reps.aggregate([{\"$project\" : {\"_id\": 0, \"objects\" : 1}},\n",
    "                       {\"$unwind\":\"$objects\"},\n",
    "                       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}\n",
    "                      ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't know your American states, join up the states collection\n",
    "joined = reps.aggregate([\n",
    "     {\"$unwind\":\"$objects\"},\n",
    "     {\"$lookup\":\n",
    "       {\n",
    "         \"from\": \"states\",\n",
    "         \"localField\": \"objects.state\",\n",
    "         \"foreignField\": \"abbreviation\",\n",
    "         \"as\": \"stateInfo\"\n",
    "       }\n",
    "  }\n",
    "])\n",
    "printDocs(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so what does AL mean for our representative\n",
    "joined = reps.aggregate([\n",
    "     {\"$unwind\":\"$objects\"},\n",
    "     {\"$lookup\":\n",
    "       {\n",
    "         \"from\": \"states\",\n",
    "         \"localField\": \"objects.state\",\n",
    "         \"foreignField\": \"abbreviation\",\n",
    "         \"as\": \"stateInfo\"\n",
    "       }},\n",
    "       {\"$match\": { \"objects.person.bioguideid\": 'A000055' }}\n",
    "])\n",
    "printDocs(joined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the senator data is similarly structured, lets find any female senators\n",
    "docs = senators.aggregate([{\"$project\" : {\"_id\": 0, \"objects.person.firstname\" : 1, \"objects.person.lastname\": 1,\n",
    "                                     \"objects.person.gender\":1}},\n",
    "                       {\"$unwind\":\"$objects\"},\n",
    "                       {\"$match\": { \"objects.person.gender\": 'female' }}\n",
    "                      ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the Senior Senator for Michigan\n",
    "\n",
    "docs = senators.aggregate([{\"$project\" : {\"_id\": 0, \"objects\":1}},\n",
    "                       {\"$unwind\":\"$objects\"},\n",
    "                       {\"$match\": { \"objects.description\": \"Senior Senator for Michigan\"}}\n",
    "                      ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally remember, due to no schema you can give an unknown field, which it will just ignore and not warn you!\n",
    "docs = senators.aggregate([{\"$project\" : {\"_id\": 0, \"objects\":1}},\n",
    "                       {\"$unwind\":\"$objects\"},\n",
    "                       {\"$match\": { \"objects.person.description\": \"Senior Senator for Michigan\"}}\n",
    "                      ])\n",
    "printDocs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-student"
    ]
   },
   "source": [
    "## Twitter Data\n",
    "\n",
    "Another example of semi-structured data is Twitter, or X data. \n",
    "\n",
    "Unfortunately, X now imposes a cost of at least $100 a month if you want to extract tweets (creating is still free!). Below are some example of tweets extracted on the 10th and 11th January 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for Twitter data\n",
    "import string\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good practice to examine the data before importing it\n",
    "! head data/BBCNews-230110-2118.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail data/BBCNews-230111-2214.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_database('twitterDB')\n",
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load this into a twitterDB database and news from 11th January\n",
    "! mongoimport --db twitterDB  --file data/BBCNews-230110-2118.json --collection bbcnews\n",
    "! mongoimport --db twitterDB  --file data/BBCNews-230111-2214.json --collection bbcnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change database\n",
    "db = client.twitterDB\n",
    "bbcnews = db.bbcnews\n",
    "bbcnews.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What columns/keys does it have\n",
    "# Some of these keys are subdocuments, such as the entities one seen above\n",
    "bbcnews.find_one().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prince Harry was topical this time last year! Is he mentioned at all?!\n",
    "# $regex allows pattern matching. The 'i' option makes the search case insensitive\n",
    "# \"_id:\" 0 suppresses showing the object id\n",
    "# SELECT text FROM bbcnews WHERE LOWER(text) LIKE '%harry%';\n",
    "\n",
    "tweets = bbcnews.find({'text':{'$regex':'Harry', '$options': 'i'}}, {\"_id\":0,'text': 1})\n",
    "printDoc(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $regex can be used on more than one field - can either use the \"OR\" clause to get either value. \n",
    "# Just make sure the brackets are the correct ones and lined up correctly!\n",
    "# SELECT text, created_at from bbcnews WHERE LOWER(text) LIKE '%Harry%' OR created_at LIKE '%Wednesday%'\n",
    "\n",
    "list(bbcnews.find({\n",
    "    \"$or\": \n",
    "    [ {'text': {'$regex':'Harry', '$options': 'i'}},  \n",
    "      {\"created_at\" : {'$regex': 'Wednesday'}} \n",
    "    ]\n",
    "    }, \n",
    "    {\"_id\":0,'created_at': 1, 'text': 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use the \"AND\" clause to get both value. \n",
    "# SELECT text, created_at from bbcnews WHERE LOWER(text) LIKE '%Harry%' AND created_at LIKE '%Wednesday%'\n",
    "\n",
    "list(bbcnews.find({\n",
    "    \"$and\": \n",
    "    [ {'text': {'$regex':'Harry', '$options': 'i'}},  \n",
    "      {\"created_at\" : {'$regex': 'Wednesday'}} \n",
    "    ]\n",
    "    }, \n",
    "    {\"_id\":0,'created_at': 1, 'text': 1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distinct languages found in the tweets\n",
    "# SELECT DISTINCT lang FROM bbcnews;\n",
    "# The supported languages can be found here: https://developer.twitter.com/en/docs/twitter-for-websites/supported-languages \n",
    "db.bbcnews.distinct(\"lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tweets have been retweeted more than 100 times\n",
    "# Use the dot notation to reference keys in any subdocument\n",
    "db.bbcnews.count_documents({\"public_metrics.retweet_count\": { '$gt' : 100 }})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = db.bbcnews.find({'entities.urls.title':{'$regex':'Firefighter'}}, {'entities.urls.title': 1})\n",
    "\n",
    "printDoc(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some fields from the entities subdocument.\n",
    "# When showing the subdocuments pretty print makes the tweets more readable\n",
    "tweets = db.bbcnews.find({}, {\"_id\":0, \"entities.urls.title\": 1, \"entities.urls.description\": 1})\n",
    "\n",
    "printDocs(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These can be searched too - find the Seal story\n",
    "tweets = db.bbcnews.find({\"entities.urls.title\": {\"$regex\": \"seal\", '$options': 'i' }}, \n",
    "                         {\"_id\":0, \"entities.urls.title\": 1, \"entities.urls.description\": 1})\n",
    "\n",
    "printDocs(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to unpack the nested documents\n",
    "# https://stackoverflow.com/questions/25909927/mongodb-how-to-get-a-field-sub-document-from-a-document\n",
    "tweets=db.bbcnews.aggregate([\n",
    "    # De-normalize the array content first\n",
    "    { \"$unwind\": \"$entities\" },\n",
    "\n",
    "    # De-normalize the content from the inner array as well\n",
    "    { \"$unwind\": \"$entities.urls\" },\n",
    "\n",
    "    # Group the \"entities\" per document\n",
    "    { \"$group\": {\n",
    "        \"_id\": \"$_id\",\n",
    "        \"entities\": { \"$addToSet\": \"$entities.urls\" }\n",
    "    }}\n",
    "])\n",
    "printDocs(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "style-activity"
    ]
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This and the relationalDB Notebooks give you a flavour of the two types of database management system. \n",
    "\n",
    "What are the differences?\n",
    "\n",
    "Some things to think about:\n",
    "\n",
    "*Relational*\n",
    "- relational has a fixed schema\n",
    "- the data is normalised, with less duplication\n",
    "- constraints can be enforced\n",
    "- ACID transaction support (Atomic, Consistency, Isolation and Durability)\n",
    "\n",
    "*NoSQL (Document)*\n",
    "- flexible schema, optional data can be easily incorporated.\n",
    "- can support agile development\n",
    "- data is denormalised, so can mean more duplication\n",
    "- constraints not enforced\n",
    "- BASE transaction support (Basically Available, Soft state, Eventual consistency!)\n",
    "\n",
    "Bear in mind that NoSQL is a relatively new technology, so can be seen as immature in that it does not provide good support for transaction handling, or access control, but could be argued that this is not the market it is aimed at. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
